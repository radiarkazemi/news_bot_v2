"""
Advanced news detection service for identifying news content in Telegram messages.
Optimized for Persian/English war and geopolitical news.
"""
import logging
import re

logger = logging.getLogger(__name__)

try:
    from config.settings import NEW_ATTRIBUTION
except ImportError:
    NEW_ATTRIBUTION = "@anilgoldgallerynews"


class NewsDetector:
    """Advanced news detector for Telegram messages with focus on war/geopolitical content."""

    # Comprehensive financial and geopolitical news keywords (Persian & English)
    FINANCIAL_NEWS_KEYWORDS = [
        # War and conflict terminology
        "Ø¬Ù†Ú¯", "Ø­Ù…Ù„Ù‡", "Ø¨Ù…Ø¨Ø§Ø±Ø§Ù†", "Ù…ÙˆØ´Ú©", "ØªÙ†Ø´", "Ø¯Ø±Ú¯ÛŒØ±ÛŒ", "Ø¹Ù…Ù„ÛŒØ§Øª Ù†Ø¸Ø§Ù…ÛŒ", "ØªÙ‡Ø¯ÛŒØ¯",
        "war", "attack", "bombing", "missile", "conflict", "military", "strike", "threat",
        
        # Israel-Iran specific terms
        "Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„", "Ø§ÛŒØ±Ø§Ù†", "Ø­Ø²Ø¨â€ŒØ§Ù„Ù„Ù‡", "Ø­Ù…Ø§Ø³", "Ù†ØªØ§Ù†ÛŒØ§Ù‡Ùˆ", "Ø±Ù‡Ø¨Ø± Ø§Ù†Ù‚Ù„Ø§Ø¨", "Ø³Ù¾Ø§Ù‡",
        "israel", "iran", "hezbollah", "hamas", "netanyahu", "irgc", "gaza", "lebanon",
        
        # Nuclear and sanctions
        "ØªØ­Ø±ÛŒÙ…", "Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ", "Ø§ÙˆØ±Ø§Ù†ÛŒÙˆÙ…", "ØºÙ†ÛŒâ€ŒØ³Ø§Ø²ÛŒ", "Ø¨Ø±Ø¬Ø§Ù…", "Ø¢Ú˜Ø§Ù†Ø³ Ø§ØªÙ…ÛŒ", "Ù†Ø·Ù†Ø²",
        "sanctions", "nuclear", "uranium", "enrichment", "jcpoa", "iaea", "natanz",
        
        # Economic warfare terms
        "Ø¯Ù„Ø§Ø±", "Ø·Ù„Ø§", "Ù†ÙØª", "Ø¨ÙˆØ±Ø³", "Ø§Ù‚ØªØµØ§Ø¯", "ØªÙˆØ±Ù…", "Ø¨Ø­Ø±Ø§Ù† Ø§Ù‚ØªØµØ§Ø¯ÛŒ", "Ù‚ÛŒÙ…Øª Ù†ÙØª",
        "economic crisis", "oil price", "gold", "dollar", "economic warfare", "inflation",
        
        # Geopolitical actors
        "Ø¢Ù…Ø±ÛŒÚ©Ø§", "Ø§Ø±ÙˆÙ¾Ø§", "Ø±ÙˆØ³ÛŒÙ‡", "Ú†ÛŒÙ†", "Ù†Ø§ØªÙˆ", "Ø¹Ø±Ø¨Ø³ØªØ§Ù†", "ØªØ±Ú©ÛŒÙ‡", "Ù…ØµØ±",
        "usa", "america", "europe", "russia", "china", "nato", "saudi", "turkey",
        
        # Military and defense
        "Ù¾Ù‡Ù¾Ø§Ø¯", "Ø¬Ù†Ú¯Ù†Ø¯Ù‡", "Ù†Ø§ÙˆØ´Ú©Ù†", "Ù¾Ø¯Ø§ÙÙ†Ø¯", "Ø³Ø§Ù…Ø§Ù†Ù‡ Ø¯ÙØ§Ø¹ÛŒ", "Ø±Ø§Ø¯Ø§Ø±", "ÙØ±ÛŒÚ¯Ø§Øª",
        "drone", "fighter", "destroyer", "defense system", "radar", "frigate",
        
        # News format indicators
        "ÙÙˆØ±ÛŒ", "Ø®Ø¨Ø±", "Ú¯Ø²Ø§Ø±Ø´", "Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´", "Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ", "ØªØ£ÛŒÛŒØ¯ Ø´Ø¯", "Ø§Ø¹Ù„Ø§Ù… Ø´Ø¯",
        "breaking", "news", "report", "confirmed", "announced", "sources say",
        
        # Regional terms
        "Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡", "Ø®Ù„ÛŒØ¬ ÙØ§Ø±Ø³", "Ù…Ù†Ø·Ù‚Ù‡", "ØªÙ†Ú¯Ù‡ Ù‡Ø±Ù…Ø²", "Ø¯Ø±ÛŒØ§ÛŒ Ø³Ø±Ø®", "Ù…Ø¯ÛŒØªØ±Ø§Ù†Ù‡",
        "middle east", "persian gulf", "region", "strait of hormuz", "red sea",
        
        # International organizations
        "Ø³Ø§Ø²Ù…Ø§Ù† Ù…Ù„Ù„", "Ø´ÙˆØ±Ø§ÛŒ Ø§Ù…Ù†ÛŒØª", "Ø§ØªØ­Ø§Ø¯ÛŒÙ‡ Ø§Ø±ÙˆÙ¾Ø§", "Ø§ÙˆÙ¾Ú©", "Ú¯Ø±ÙˆÙ‡ Ù‡ÙØª",
        "united nations", "security council", "european union", "opec", "g7"
    ]

    # Non-news content indicators
    NON_NEWS_KEYWORDS = [
        # Commercial content
        "ØªØ¨Ù„ÛŒØº", "ÙØ±ÙˆØ´", "Ø®Ø±ÛŒØ¯", "ØªØ®ÙÛŒÙ", "ÙˆÛŒÚ˜Ù‡", "Ø±Ø³ØªÙˆØ±Ø§Ù†", "Ú©Ø§ÙÙ‡", "Ù‡ØªÙ„",
        "advertisement", "sale", "buy", "discount", "special", "restaurant", "cafe",
        
        # Entertainment
        "Ø³Ø±Ú¯Ø±Ù…ÛŒ", "ÙÛŒÙ„Ù…", "Ø³ÛŒÙ†Ù…Ø§", "Ù…ÙˆØ³ÛŒÙ‚ÛŒ", "Ú©Ù†Ø³Ø±Øª", "Ø¨Ø§Ø²ÛŒÚ¯Ø±", "Ø®ÙˆØ§Ù†Ù†Ø¯Ù‡",
        "entertainment", "movie", "cinema", "music", "concert", "actor", "singer",
        
        # Sports
        "ÙˆØ±Ø²Ø´", "ÙÙˆØªØ¨Ø§Ù„", "ÙˆØ§Ù„ÛŒØ¨Ø§Ù„", "Ø¨Ø³Ú©ØªØ¨Ø§Ù„", "ØªÙ†ÛŒØ³", "Ø´Ù†Ø§", "Ø¯ÙˆÚ†Ø±Ø®Ù‡â€ŒØ³ÙˆØ§Ø±ÛŒ",
        "sports", "football", "soccer", "volleyball", "basketball", "tennis",
        
        # Personal/social
        "ØªÙˆÙ„Ø¯", "Ø¹Ø±ÙˆØ³ÛŒ", "Ø³ÙØ±", "ØªØ¹Ø·ÛŒÙ„Ø§Øª", "Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡", "Ø¯ÙˆØ³ØªØ§Ù†", "Ø´Ø®ØµÛŒ",
        "birthday", "wedding", "travel", "vacation", "family", "friends", "personal",
        
        # Weather and routine
        "Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§", "Ù‡ÙˆØ§Ø´Ù†Ø§Ø³ÛŒ", "Ø¨Ø§Ø±Ø´", "Ø¨Ø±Ù", "Ú¯Ø±Ù…Ø§", "Ø³Ø±Ù…Ø§", "ØªØ±Ø§ÙÛŒÚ©",
        "weather", "forecast", "rain", "snow", "traffic", "temperature"
    ]

    @classmethod
    def is_news(cls, text):
        """Enhanced news detection with pattern matching."""
        if not text or len(text.strip()) < 30:
            logger.debug("Text too short for news detection")
            return False

        text_lower = text.lower()
        
        # Count relevant keywords with weighted scoring
        relevant_term_count = 0
        high_priority_matches = 0
        
        for keyword in cls.FINANCIAL_NEWS_KEYWORDS:
            if keyword.lower() in text_lower:
                relevant_term_count += 1
                # Extra weight for war/conflict terms
                if keyword in ["Ø¬Ù†Ú¯", "Ø­Ù…Ù„Ù‡", "Ù…ÙˆØ´Ú©", "ØªØ­Ø±ÛŒÙ…", "war", "attack", "missile", "sanctions"]:
                    high_priority_matches += 1
        
        # Count non-news indicators
        non_news_count = sum(1 for keyword in cls.NON_NEWS_KEYWORDS 
                           if keyword.lower() in text_lower)
        
        # News format indicators with regex patterns
        format_indicators = [
            r'ÙÙˆØ±ÛŒ[::\s]',          # "ÙÙˆØ±ÛŒ:"
            r'Ø®Ø¨Ø± ÙÙˆØ±ÛŒ[::\s]',      # "Ø®Ø¨Ø± ÙÙˆØ±ÛŒ:"
            r'Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´',             # "Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´"
            r'Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ',           # "Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ"
            r'ØªØ£ÛŒÛŒØ¯ Ø´Ø¯',            # "ØªØ£ÛŒÛŒØ¯ Ø´Ø¯"
            r'Ø§Ø¹Ù„Ø§Ù… Ø´Ø¯',            # "Ø§Ø¹Ù„Ø§Ù… Ø´Ø¯"
            r'Ú¯Ø²Ø§Ø±Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯',         # "Ú¯Ø²Ø§Ø±Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯"
            r'breaking[::\s]', 
            r'urgent[::\s]',
            r'news[::\s]',
            r'report[::\s]',
            r'confirmed[::\s]',
            r'announced[::\s]'
        ]
        
        format_indicator_count = sum(1 for pattern in format_indicators 
                                   if re.search(pattern, text_lower))
        
        # High-priority news pattern detection
        priority_patterns = [
            r'ÙÙˆØ±ÛŒ.*?Ø­Ù…Ù„Ù‡',         # "ÙÙˆØ±ÛŒ: Ø­Ù…Ù„Ù‡"
            r'Ù…ÙˆØ´Ú©.*?Ø´Ù„ÛŒÚ©',        # "Ù…ÙˆØ´Ú© Ø´Ù„ÛŒÚ© Ø´Ø¯"
            r'ØªØ­Ø±ÛŒÙ….*?Ø¬Ø¯ÛŒØ¯',        # "ØªØ­Ø±ÛŒÙ… Ø¬Ø¯ÛŒØ¯"
            r'Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„.*?Ø­Ù…Ù„Ù‡',      # "Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ Ø­Ù…Ù„Ù‡ Ú©Ø±Ø¯"
            r'Ø§ÛŒØ±Ø§Ù†.*?Ø§Ø¹Ù„Ø§Ù…',       # "Ø§ÛŒØ±Ø§Ù† Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯"
            r'breaking.*?news',
            r'urgent.*?update',
            r'missile.*?attack',
            r'israel.*?strikes',
            r'iran.*?announces'
        ]
        
        priority_pattern_count = sum(1 for pattern in priority_patterns 
                                   if re.search(pattern, text_lower))
        
        # Decision logic with multiple criteria
        logger.debug(f"News analysis: relevant_terms={relevant_term_count}, "
                    f"high_priority={high_priority_matches}, "
                    f"format_indicators={format_indicator_count}, "
                    f"priority_patterns={priority_pattern_count}, "
                    f"non_news={non_news_count}")
        
        # Strong news indicators
        if priority_pattern_count >= 1:
            logger.debug("Detected as news: priority pattern match")
            return True
            
        if high_priority_matches >= 2 and format_indicator_count >= 1:
            logger.debug("Detected as news: multiple high-priority terms with format")
            return True
            
        if relevant_term_count >= 4 and non_news_count == 0:
            logger.debug("Detected as news: high relevant term count, no non-news terms")
            return True
            
        if relevant_term_count >= 2 and format_indicator_count >= 2:
            logger.debug("Detected as news: relevant terms with strong format indicators")
            return True
            
        # For longer texts with decent relevance
        if len(text) > 150 and relevant_term_count >= 2 and format_indicator_count >= 1:
            logger.debug("Detected as news: long text with relevant content")
            return True
            
        # Reject if too many non-news indicators
        if non_news_count >= 2:
            logger.debug("Rejected as news: too many non-news indicators")
            return False
            
        logger.debug("Not detected as news")
        return False

    @classmethod
    def clean_news_text(cls, text):
        """Clean and format news text for professional redistribution."""
        if not text:
            return ""
            
        logger.debug(f"Cleaning news text (length: {len(text)})")
        
        # Step 1: Normalize whitespace and line breaks
        cleaned = re.sub(r'\n{3,}', '\n\n', text.strip())
        cleaned = re.sub(r'\r\n', '\n', cleaned)
        cleaned = re.sub(r'\r', '\n', cleaned)
        
        # Step 2: Remove Telegram-specific elements
        cleaned = re.sub(r'@\w+\b', '', cleaned)  # Remove handles
        cleaned = re.sub(r'https?://\S+', '', cleaned)  # Remove URLs
        cleaned = re.sub(r't\.me/\S+', '', cleaned)  # Remove t.me links
        cleaned = re.sub(r'#\w+', '', cleaned)  # Remove hashtags
        
        # Step 3: Remove excessive emojis and symbols
        cleaned = re.sub(r'[\U00010000-\U0010ffff]{3,}', 'ğŸ”¸', cleaned)  # Replace emoji spam
        cleaned = re.sub(r'[ğŸ”¸ğŸ”¹â–â¬›â¬œ]{3,}', '', cleaned)  # Remove symbol spam
        
        # Step 4: Remove common prefixes that we'll replace
        prefixes_to_remove = [
            "ÙÙˆØ±ÛŒ:", "ÙÙˆØ±ÛŒ", "Ø®Ø¨Ø± ÙÙˆØ±ÛŒ:", "Ø®Ø¨Ø±:", "Ú¯Ø²Ø§Ø±Ø´:", "Ø§Ø·Ù„Ø§Ø¹ÛŒÙ‡:",
            "Breaking:", "Urgent:", "News:", "Report:"
        ]
        for prefix in prefixes_to_remove:
            if cleaned.startswith(prefix):
                cleaned = cleaned[len(prefix):].strip()
                break
        
        # Step 5: Clean up whitespace
        cleaned = re.sub(r'\s+$', '', cleaned)  # Remove trailing whitespace
        cleaned = re.sub(r'^\s+', '', cleaned)  # Remove leading whitespace
        cleaned = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned)  # Max 2 consecutive newlines
        cleaned = re.sub(r'[ \t]+', ' ', cleaned)  # Normalize spaces
        
        # Step 6: Professional formatting
        if cleaned:
            # Determine urgency indicator
            urgency_indicator = "ğŸ“°"  # Default
            if any(urgent in text.lower() for urgent in ["ÙÙˆØ±ÛŒ", "urgent", "breaking", "Ø¹Ø§Ø¬Ù„"]):
                urgency_indicator = "ğŸ”´"
            elif any(important in text.lower() for important in ["Ù…Ù‡Ù…", "important", "Ù‡Ø´Ø¯Ø§Ø±", "warning"]):
                urgency_indicator = "ğŸ”¶"
            elif any(conflict in text.lower() for conflict in ["Ø¬Ù†Ú¯", "Ø­Ù…Ù„Ù‡", "Ù…ÙˆØ´Ú©", "war", "attack", "missile"]):
                urgency_indicator = "âš¡"
            
            # Add professional formatting with attribution
            try:
                from src.utils.time_utils import get_formatted_time
                timestamp = get_formatted_time()
                formatted_text = f"{urgency_indicator} {cleaned}\n\nğŸ“¡ {NEW_ATTRIBUTION}\nğŸ• {timestamp}"
            except ImportError:
                # Fallback if time_utils not available
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                formatted_text = f"{urgency_indicator} {cleaned}\n\nğŸ“¡ {NEW_ATTRIBUTION}\nğŸ• {timestamp}"
            
            return formatted_text
            
        return cleaned

    @classmethod
    def split_combined_news(cls, text):
        """Split combined news messages into individual news items."""
        # Enhanced separators for Persian news channels
        separators = [
            r'[-]{3,}',               # Multiple dashes: ---
            r'[=]{3,}',               # Multiple equals: ===
            r'[_]{3,}',               # Multiple underscores: ___
            r'[*]{3,}',               # Multiple asterisks: ***
            r'[.]{3,}',               # Multiple periods: ...
            r'ğŸ”¸{2,}',                # Diamond symbols
            r'ğŸ”¹{2,}',                # Diamond symbols  
            r'â–{2,}',                # Minus symbols
            r'â¬›{2,}',                # Black squares
            r'â¬œ{2,}',                # White squares
            r'ã€°ï¸{1,}',               # Wavy dashes
            r'ã€°{1,}',                # Wavy dashes alt
            r'\n\s*\n\s*\n',         # Multiple empty lines
            r'â”{3,}',                 # Heavy horizontal lines
            r'â–ª{3,}',                 # Black squares
            r'â–«{3,}',                 # White squares
            r'â– {3,}',                 # Black large squares
            r'â–¡{3,}',                 # White large squares
            r'â—{3,}',                 # Black circles
            r'â—‹{3,}',                 # White circles
            # Persian-specific patterns
            r'Ø®Ø¨Ø±\s+(?:Ø¨Ø¹Ø¯ÛŒ|Ø¯ÙˆÙ…|Ø³ÙˆÙ…)',  # "Ø®Ø¨Ø± Ø¨Ø¹Ø¯ÛŒ", "Ø®Ø¨Ø± Ø¯ÙˆÙ…"
            r'(?:Û±|Û²|Û³|Û´|Ûµ)\s*[-.)]\s*',  # Persian numbers with separators
            r'\n\s*[Û±-Û¹]+\s*[-.)]\s*'     # Numbered lists in Persian
        ]
        
        # Combine all separators
        pattern = '|'.join(separators)
        
        # Split the text
        segments = re.split(pattern, text, flags=re.IGNORECASE)
        
        # Clean and filter segments
        cleaned_segments = []
        for segment in segments:
            segment = segment.strip()
            if len(segment) >= 50:  # Only keep substantial segments
                cleaned_segments.append(segment)
        
        # If no segments found, return original as single item
        if not cleaned_segments:
            return [text]
            
        logger.debug(f"Split news into {len(cleaned_segments)} segments")
        return cleaned_segments

    @classmethod
    def extract_news_metadata(cls, text):
        """Extract metadata from news text."""
        metadata = {
            'urgency': 'normal',
            'category': 'general',
            'source': None,
            'language': 'mixed',
            'word_count': len(text.split()),
            'char_count': len(text)
        }
        
        text_lower = text.lower()
        
        # Detect urgency
        if any(urgent in text_lower for urgent in ["ÙÙˆØ±ÛŒ", "breaking", "urgent", "Ø¹Ø§Ø¬Ù„"]):
            metadata['urgency'] = 'urgent'
        elif any(important in text_lower for important in ["Ù…Ù‡Ù…", "important", "Ù‡Ø´Ø¯Ø§Ø±"]):
            metadata['urgency'] = 'important'
        
        # Detect category
        if any(war in text_lower for war in ["Ø¬Ù†Ú¯", "Ø­Ù…Ù„Ù‡", "Ù…ÙˆØ´Ú©", "war", "attack"]):
            metadata['category'] = 'war'
        elif any(econ in text_lower for econ in ["ØªØ­Ø±ÛŒÙ…", "Ø§Ù‚ØªØµØ§Ø¯", "Ø¯Ù„Ø§Ø±", "sanctions"]):
            metadata['category'] = 'economic'
        elif any(nuke in text_lower for nuke in ["Ù‡Ø³ØªÙ‡â€ŒØ§ÛŒ", "Ø§ÙˆØ±Ø§Ù†ÛŒÙˆÙ…", "nuclear"]):
            metadata['category'] = 'nuclear'
        
        # Detect source patterns
        source_patterns = [
            r'Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´\s+([^ØŒ]+)',      # "Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´ SOURCE"
            r'Ø¨Ø± Ø§Ø³Ø§Ø³\s+([^ØŒ]+)',       # "Ø¨Ø± Ø§Ø³Ø§Ø³ SOURCE"
            r'Ù…Ù†Ø§Ø¨Ø¹\s+([^ØŒ]+)',         # "Ù…Ù†Ø§Ø¨Ø¹ SOURCE"
            r'according to\s+([^,]+)',  # "according to SOURCE"
            r'reported by\s+([^,]+)'    # "reported by SOURCE"
        ]
        
        for pattern in source_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                metadata['source'] = match.group(1).strip()
                break
        
        # Detect language predominance
        persian_chars = len(re.findall(r'[\u0600-\u06FF]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        
        if persian_chars > english_chars * 2:
            metadata['language'] = 'persian'
        elif english_chars > persian_chars * 2:
            metadata['language'] = 'english'
        
        return metadata